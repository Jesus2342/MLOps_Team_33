{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 666 entries, 0 to 665\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Performance           666 non-null    object\n",
      " 1   Gender                666 non-null    object\n",
      " 2   Caste                 666 non-null    object\n",
      " 3   coaching              666 non-null    object\n",
      " 4   time                  666 non-null    object\n",
      " 5   Class_ten_education   666 non-null    object\n",
      " 6   twelve_education      666 non-null    object\n",
      " 7   medium                666 non-null    object\n",
      " 8   Class_ X_Percentage   666 non-null    object\n",
      " 9   Class_XII_Percentage  666 non-null    object\n",
      " 10  Father_occupation     666 non-null    object\n",
      " 11  Mother_occupation     666 non-null    object\n",
      "dtypes: object(12)\n",
      "memory usage: 62.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#Get route of the path\n",
    "current_path = os.getcwd()\n",
    "aux_curr_path = current_path\n",
    "project_path = aux_curr_path.replace('/notebooks', '')\n",
    "dataset_path = \"dataset/CEE_DATA.arff\"\n",
    "dataset_path = os.path.join(project_path, dataset_path)\n",
    "\n",
    "data, meta = arff.loadarff(dataset_path)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.applymap(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x) #Encoding from byte to string \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            #0 : Average -  157\n",
    "            #1 : Excellent - 101\n",
    "            #2 : Good - 210\n",
    "            #3 : Very Good - 198"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest=[\"Performance\",'Class_ X_Percentage', 'Class_XII_Percentage', 'medium', 'Caste']\n",
    "updated_df=df[columns_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= updated_df[['Performance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding y_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Create oneHot enconder object\n",
    "enc_OneHot = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "#Applying OneHot\n",
    "y_OneHot = enc_OneHot.fit_transform(y)\n",
    "\n",
    "#Create Label encoder object\n",
    "ord_enc=LabelEncoder()\n",
    "\n",
    "#Applying LabelEnconder to y\n",
    "df[\"y_ord_enc\"]=ord_enc.fit_transform(y)\n",
    "y_Label = df[\"y_ord_enc\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding y_label coded to **updated_df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Performance', 'Class_ X_Percentage', 'Class_XII_Percentage', 'medium',\n",
       "       'Caste', 'y_coding_col'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df = updated_df.assign(y_coding_col=y_Label.values)\n",
    "updated_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining OneHot tranformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns to apply one hot enconder\n",
    "col_X=['Class_ X_Percentage', 'Class_XII_Percentage', 'medium', 'Caste']\n",
    "\n",
    "#Create the transformer\n",
    "ct= ColumnTransformer(\n",
    "    transformers=[\n",
    "    (\"OneHotInXColumns\", enc_OneHot,col_X)\n",
    "                      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy without treating the inbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2drop = [\"Performance\", 'y_coding_col']\n",
    "X_no_balanced = updated_df.drop(cols2drop, axis=1)\n",
    "\n",
    "\n",
    "y_no_balanced= updated_df[['Performance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding X_no_balanced --OneHot--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying OneHot to X\n",
    "X_no_balanced_OneHot = ct.fit_transform(X_no_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding y_no_balanced --OneHot--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_OneHot_no_balanced = enc_OneHot.fit_transform(y_no_balanced)\n",
    "y_Label_no_balanced = updated_df[\"y_coding_col\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy for inbalance multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVER SAMPLING AND UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new df per class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_av_class_0 = updated_df[updated_df[\"y_coding_col\"]==0]\n",
    "df_ex_class_1 = updated_df[updated_df[\"y_coding_col\"]==1]\n",
    "df_gd_class_2 = updated_df[updated_df[\"y_coding_col\"]==2]\n",
    "df_vg_class_3 = updated_df[updated_df[\"y_coding_col\"]==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ovesampling Class 0 and class 1\n",
    "\n",
    "#### Undesampling Class 2 and Class 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Performance\n",
       "Average      180\n",
       "Excellent    180\n",
       "Good         180\n",
       "Vg           180\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Number_of_samples = 180\n",
    "\n",
    "#Oversampling\n",
    "df_av_class_0_over =  df_av_class_0.sample(Number_of_samples, replace=True)\n",
    "df_ex_class_1_over =  df_ex_class_1.sample(Number_of_samples, replace=True)\n",
    "\n",
    "#Undersampling\n",
    "df_gd_class_2_under = df_gd_class_2.sample(Number_of_samples)\n",
    "df_vg_class_3_under = df_vg_class_3.sample(Number_of_samples)\n",
    "\n",
    "df_mod_samples = pd.concat([df_av_class_0_over,\n",
    "                            df_ex_class_1_over,\n",
    "                            df_gd_class_2_under,\n",
    "                            df_vg_class_3_under],axis=0) \n",
    "\n",
    "df_mod_samples.Performance.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -X will called as X_balan (180 samples per class)\n",
    "\n",
    "### -y will called as y_balan (180 samples per class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2drop = [\"Performance\", 'y_coding_col']\n",
    "X_balan = df_mod_samples.drop(cols2drop, axis=1)\n",
    "\n",
    "\n",
    "y_balan = df_mod_samples[[\"Performance\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding X_balan --OneHot--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying OneHot to X\n",
    "X_balan = ct.fit_transform(X_balan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding y_balan --OneHot--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying OneHot\n",
    "y_OneHot_balan = enc_OneHot.fit_transform(y_balan)\n",
    "\n",
    "#Applying LabelEnconder to y\n",
    "y_Label_balan = df_mod_samples[\"y_coding_col\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote= SMOTE(sampling_strategy=\"minority\")\n",
    "X_sm, y_sm =  smote.fit_resample(X_no_balanced_OneHot,y_Label_no_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding y_balan --OneHot--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sm_array = np.array(y_sm)\n",
    "y_sm_df = pd.DataFrame(y_sm_array, columns=['label_smote_Performance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying OneHot\n",
    "y_sm_OneHot = enc_OneHot.fit_transform(y_sm_df)\n",
    "y_sm_label=y_sm_df[\"label_smote_Performance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BALANCED SPLIT - Over Sampling and Under Sampling \n",
    "\n",
    "#Split the dataset for y_OneHot_balan - Balanced applying over and under sampling  -- 720 samples, 180 samples per class\n",
    "X_train_balan, X_test_balan, y_train_OneHot_balan, y_test_OneHot_balan = train_test_split(X_balan, y_OneHot_balan, test_size=0.2, random_state=42)\n",
    "\n",
    "#Split the dataset for y_Label (Pandas Series) --Not Balanced\n",
    "X_train_balan, X_test_balan, y_train_Label_balan, y_test_Label_balan = train_test_split(X_balan, y_Label_balan, test_size=0.2, random_state=42)\n",
    "\n",
    "### BALANCED SPLIT - SMOTE\n",
    "\n",
    "\n",
    "###  --NOT BALANCED SPLIT--\n",
    "\n",
    "#Split the dataset for y_OneHot -- Not balanced -- 666 samples \n",
    "X_train_no_balanced, X_test_no_balanced, y_train_OneHot_no_balanced, y_test_OneHot_no_balanced = train_test_split(X_no_balanced_OneHot, y_OneHot_no_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "#Split the dataset for y_Label (Pandas Series) --Not Balanced\n",
    "X_train_no_balanced, X_test_no_balanced, y_train_Label_no_balanced, y_test_Label_no_balanced = train_test_split(X_no_balanced_OneHot, y_Label_no_balanced, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Experiments = [\n",
    "    (\n",
    "        \"Random Forest n_estimators=100\", \n",
    "        RandomForestClassifier(class_weight=\"balanced\"),\n",
    "        (X_train, y_train_OneHot),\n",
    "        (X_test, y_test_OneHot)\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"XGBoost\",\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        (X_train, y_train_OneHot),\n",
    "        (X_test, y_test_OneHot)\n",
    "    ),\n",
    "    \n",
    "    (\n",
    "         \"Multinomial Logistic Regression\",\n",
    "        LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200),\n",
    "        (X_train, y_train_Label),\n",
    "        (X_test, y_test_Label)\n",
    "    ),\n",
    "    \n",
    "     (\n",
    "        \"K-Nearest Neighbors\",\n",
    "        KNeighborsClassifier(n_neighbors=5),\n",
    "        (X_train, y_train_Label),\n",
    "        (X_test, y_test_Label)\n",
    "    ),\n",
    "    \n",
    "    (\n",
    "        \"MLP\",\n",
    "        MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, activation='relu', solver='adam'),\n",
    "        (X_train, y_train_OneHot),\n",
    "        (X_test, y_test_OneHot)\n",
    "    ),\n",
    "    \n",
    "    (\n",
    "        \"Support Vector Classifier\",\n",
    "        SVC(kernel='linear', probability=True),  \n",
    "        (X_train, y_train_Label),\n",
    "        (X_test, y_test_Label)\n",
    "    )\n",
    "    \n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_model = []\n",
    "\n",
    "for model_name, model, train_set, test_set in Experiments:\n",
    "    X_train = train_set[0] #get Xtrain from the list models\n",
    "    y_train = train_set[1] #get y_train from list models\n",
    "    X_test = test_set[0]    #get x_test from list models \n",
    "    y_test = test_set[1]   #get y_test from list models\n",
    "    \n",
    "    model.fit(X_train, y_train)  #train the current model\n",
    "    y_pred = model.predict(X_test) #make predictions \n",
    "    report = classification_report(y_test, y_pred, output_dict=True) #make a dict of the classification report\n",
    "    \n",
    "    \n",
    "    results_per_model.append(report) #add the previus dict to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish experiements to server "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "mlflow.set_experiment(\"Student Performance Analysis Model\")\n",
    "mlflow.set_tracking_uri(\"http://3.84.228.208:5000\")\n",
    "\n",
    "for i, element in enumerate(Experiments):\n",
    "    model_name = element[0]\n",
    "    model = element[1]\n",
    "    report = results_per_model[i]\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):        \n",
    "            mlflow.log_param(\"model\", model_name)\n",
    "            \n",
    "            \n",
    "            # -------------Class interpretation---------------- \n",
    "            #0 : Average\n",
    "            #1 : Excellent\n",
    "            #2 : Good\n",
    "            #3 : Very Good\n",
    "\n",
    "            #Metrics of class 0\n",
    "            \n",
    "            mlflow.log_metric('acurracy_class_0', report['0']['precision'])\n",
    "            mlflow.log_metric('recall_class_0', report['0']['recall'])\n",
    "            mlflow.log_metric('f1_class_0', report['0']['f1-score'])\n",
    "            \n",
    "            #Metrics of class 1\n",
    "             \n",
    "            mlflow.log_metric('acurracy_class_1', report['1']['precision'])\n",
    "            mlflow.log_metric('recall_class_1', report['1']['recall'])\n",
    "            mlflow.log_metric('f1_class_1', report['1']['f1-score'])\n",
    "            \n",
    "            #Metrics of class 2\n",
    "            \n",
    "            mlflow.log_metric('acurracy_class_2', report['2']['precision'])\n",
    "            mlflow.log_metric('recall_class_2', report['2']['recall'])\n",
    "            mlflow.log_metric('f1_class_2', report['2']['f1-score'])\n",
    "            \n",
    "            #Metrics of class 3\n",
    "            \n",
    "            mlflow.log_metric('acurracy_class_3', report['3']['precision'])\n",
    "            mlflow.log_metric('recall_class_3', report['3']['recall'])\n",
    "            mlflow.log_metric('f1_class_3', report['3']['f1-score'])\n",
    "            \n",
    "        \n",
    "            if \"XGB\" in model_name:\n",
    "                mlflow.xgboost.log_model(model, \"model\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, \"model\") \n",
    "                \n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
